{
  "hash": "12674fbcb0591d78f614c51f851834c7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: (Pretty) big data wrangling with DuckDB and Polars\nsubtitle: With examples in R and Python\nformat:\n  clean-revealjs:\n    self-contained: true\nauthor:\n  - name: Grant McDermott\n    url: https://grantmcdermott.com/duckdb-polars\n    email: gmcd@amazon.com\n    affiliations:\n      - Principal Economist, Amazon\n    orcid: 0000-0001-7883-8573\ndate: May 1, 2024\n# date: last-modified\nexecute: \n  cache: true\n---\n\n\n\n\n## Preliminaries\n\n### Agenda and expectations\n\nThese sparse slides are mostly intended to serve as a rough guide map.\n\n- Most of what we'll be doing is live coding and working through examples.\n- I _strongly_ encourage you try these examples on you own machines. Laptops are perfectly fine. \n\n**Note:** All of the material for today's workshop are available on my website:\n\n- **<https://grantmcdermott.com/duckdb-polars>**\n\n## Preliminaries\n\n### Requirements\n\n**Important:** Before continuing, please make sure that you have completed the\n[requirements](https://grantmcdermott.com/duckdb-polars/requirements.html)\nlisted on the workshop website.\n\n- Install the required R and/or Python libraries.\n- Download some NYC taxi data.\n\nThe data download step can take 15-20 minutes, depending on your internet\nconnection.\n\n## Problem statement \n### Why this workshop?\n\nIt's a trope, but \"big data\" is everywhere. This is true whether you work in tech (like I do now), or in academic research (like I used to).\n\nOTOH many of datasets that I find myself working with aren't at the scale of truly _huge_ data that might warrant a Spark cluster.\n\n- We're talking anywhere between 100 MB to 50 GB. (Max a few billion rows; often in the millions or less.)\n- Can I do my work without the pain of going through Spark?\n\nAnother factor is working in polyglot teams. It would be great to repurpose similar syntax and libraries across languages...\n\n\n## Taster\n### DuckDB example\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\nnyc = open_dataset(here::here(\"taxi-data\"))\nprettyNum(nrow(nyc), \",\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"178,544,324\"\n```\n\n\n:::\n\n```{.r .cell-code}\ntic = Sys.time()\n\nnyc_summ = nyc |>\n  to_duckdb() |>\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  ) |> \n  collect()\n\n(toc = Sys.time() - tic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 0.8269699 secs\n```\n\n\n:::\n:::\n\n\n\n\n## Taster\n### DuckDB example (cont.)\n\nWe just read a ~180 million row dataset (from disk!) and did a group-by aggregation on it. \n\nIn < 1 second. \n\nOn a laptop.\n\nü§Ø\n\n:::{.fragment}\n<br>\nLet's do a quick horesrace comparison (similar grouped aggregation, but on a slightly smaller dataset)...\n:::\n\n## Simple benchmark: Computation time only\n### DuckDB and Polars are already plenty fast...\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/benchmark_plot1-1.png){width=960}\n:::\n:::\n\n\n\n\n## Simple benchmark: Computation time + data I/O\n### ... but are even more impressive once we account for data import times\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides_files/figure-revealjs/benchmark_plot2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Wait. How??\n### Better disk storage ü§ù Better memory representation\n\nTwo coinciding (r)evolutions enable faster, smarter computation:\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n#### 1. Better on-disk storage\n\n- Mainly talking about the [Parquet](https://parquet.apache.org/) file format here.\n- Columnar storage format allows better compression (much smaller footprint) and efficient random access to selected rows or columns (don't have to read the whole dataset _a la_ CSVs).\n\n:::\n\n::: {.column width=\"50%\"}\n\n#### 2. Better in-memory representation\n\n- Standardisation around the [Apache Arrow](https://arrow.apache.org/) format + columnar representation. (Allows zero copy, fewer cache misses, etc.)\n- [OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing) + deferred [materialisation](https://en.wikipedia.org/wiki/Materialized_view). (Rather than \"eagerly\" executing each query step, we can be \"lazy\" and optimise queries before executing them.)\n\n:::\n\n::::\n\n## Scaling up\n### Even moar benchmarks\n\n**Question:** Do these benchmarks hold and scale more generally? **Answer:** Yes. See [_Database-like ops benchmark_](https://duckdblabs.github.io/db-benchmark/){preview-link=\"true\"}.\n\nMoreover---and I think this is key---these kinds of benchmarks normally exclude the data I/O component... and the associated benefits of not having to hold the whole dataset in RAM.\n\n- There are some fantastically fast in-memory data wrangling libraries out there. (My personal faves: [data.table](https://github.com/Rdatatable/data.table) and [collapse](https://github.com/SebKrantz/collapse).) But \"in-memory\" means that you always have to keep the full dataset in, well, memory. And this can be expensive.\n- Libraries like DuckDB and Polars sidestep this problem, effectively supercharging your computer's data wrangling powers.\n\n\n## Examples\n### Live coding sessions\n\nLet's head back to the website to work through some notebooks.\n\n#### DuckDB\n\n - [DuckDB SQL](../duckdb-sql.qmd)\n - [DuckDB + dplyr (R)](../duckdb-dplyr.qmd)\n - [DuckDB + Ibis (Python)](../duckdb-ibis.qmd)\n\n#### Polars\n\n - [Polars from R and Python](../polars-rpy.qmd)\n\n## What didn't we cover?\n### Other cool features\n\n- **S3 I/O**\n    - DuckDB & Polars can both read/write directly from/to S3. You just need to provision your AWS credentials. [Ex. [1](https://drive-render.corp.amazon.com/view/gmcd@/codebook/devdesk.html#sec-rs3), [2](https://cboettig.github.io/duckdbfs/), [3](https://medium.com/@louis_10840/how-to-process-data-stored-in-amazon-s3-using-polars-2305bf064c52)]\n    - Note: I prefer/recommend the workflow we practiced today---first download to local disk via `aws cli`---to avoid network + I/O latency.\n\n- **Geospatial** \n    - IMO the next iteration of geospatial computation will be built on top of the tools we've seen today (and related libs).\n    - DuckDB provides an excellent [spatial extension](https://github.com/duckdblabs/duckdb_spatial) (works with [dplyr](https://cboettig.github.io/duckdbfs/)). See also the [GeoParquet](https://geoparquet.org/), [GeoArrow](https://geoarrow.org/), & [GeoPolars](https://geopolars.org/latest/) initiatives.\n\n## What didn't we cover?\n### Other cool features (cont.)\n\n- **Streaming**\n    - [Streaming](https://pola-rs.github.io/polars-book/user-guide/concepts/streaming/) is the feature that enables working with bigger-than-RAM data.\n    - Very easy to use and/or adjust our workflow to these cases...\n    - DuckDB: Simply specify a disk-backed database when you first fire up your connection from Python or R, e.g.\n    ```r\n    con = dbConnect(duckdb(), dbdir = \"nyc.dbb\")\n    ```\n    - Polars: Simply specify streaming when collecting, e.g.\n    ```py\n    some_query.collect(streaming=True)\n    ```\n\n## What didn't we cover?\n### Other cool features (cont.)\n\n- **Modeling**\n  - The modeling part of this workflow is less tightly integrated b/c we generally have to bring the data into RAM.\n  - But being able to quickly I/O parts of large datasets makes it very easy to iteratively run analyses on subsets of your data. E.g., I typically pair with [**fixest**](https://lrberge.github.io/fixest/) for unmatched performance on high-dimensional data.\n  - You can also run bespoke models via UDFs and/or predictions on database backends. [Ex. [1](https://duckdb.org/2023/07/07/python-udf.html), [2](https://posit-conf-2023.github.io/arrow/materials/4_data_manipulation_2.html#/user-defined-functions-aka-udfs), [3](https://tidypredict.tidymodels.org/)]\n  - FWIW I believe that the underlying matrix and linear algebra libraries for _direct_ modeling with these tools are coming. [Ex. [1](https://arrow.apache.org/docs/cpp/api/tensor.html), [2](https://substrait.io/extensions/functions_arithmetic/)]\n\n## Resources\n### Learning more\n\n#### DuckDB\n\n- [DuckDB homepage](https://duckdb.org/). Includes a very informative [blog](https://duckdb.org/news/) and standalone documentation for the client APIs ([Python](https://duckdb.org/docs/archive/0.8.1/api/python/overview), [R](https://duckdb.org/docs/api/r.html), and many others).\n- Also check out [Harlequin](https://harlequin.sh/) for a cool, shell-based DuckDB IDE.\n\n#### Polars\n\n- [Polars GitHub Repo](https://github.com/pola-rs/polars). Contains links to the standalone documentation for the client APIS ([Python](https://pola-rs.github.io/polars/py-polars/html/reference/index.html), [R](https://rpolars.github.io/index.html), etc.)\n- Side-by-side code comparisons (versus pandas, dplyr, etc.) are available in [*Modern Polars (in Python)*](https://kevinheavey.github.io/modern-polars/) and [*Codebook for Polars in R*](https://ddotta.github.io/cookbook-rpolars/).\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
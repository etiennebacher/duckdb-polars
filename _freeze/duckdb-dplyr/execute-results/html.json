{
  "hash": "f52c2ab98c53a33cb5217b72dd99ab1a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: DuckDB + dplyr (R)\nsubtitle: Use a familiar R frontend\nexecute:\n  freeze: auto\n  cache: true\n---\n\n\n## Load libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckdb)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: DBI\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr, warn.conflicts = FALSE)\n```\n:::\n\n\n## Create a database connection\n\nFor the `d(b)plyr` workflow, the connection step is very similar to the pure SQL\napproach. The only difference is that, after instantiating the database\nconnection, we need to register our parquet dataset as a table in our connection\nvia the `dplyr::tbl()` function. Note that we also assign it to an object (here:\n`nyc`) that can be referenced from R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Instantiate the in-memory DuckDB connection \ncon = dbConnect(duckdb(), shutdown = TRUE)\n\n## Register our parquet dataset as table in our connection (and that assign it\n## to an object that R understands)\n# nyc = tbl(con, \"nyc-taxi/**/*.parquet\") # works, but safer to use the read_parquet func)\nnyc = tbl(con, \"read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)\")\n```\n:::\n\n\n## First example\n\nThis next command runs instantly because all computation is deferred (i.e.,\nlazy eval). In other words, it is just a query object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq1 = nyc |>\n  summarize(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n```\n:::\n\n\n:::{.callout-tip}\n## `.by` versus `group_by` \nIn case you weren't aware: `summarize(..., .by = x)` is a shorthand (and\nnon-persistent) version of `group_by(x) |> summarize(...)`. More details\n[here](https://www.tidyverse.org/blog/2023/02/dplyr-1-1-0-per-operation-grouping/).\n:::\n\nWe can see what DuckDB's query tree looks like by asking it to explain\nthe plan\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexplain(q1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<SQL>\nSELECT passenger_count, AVG(tip_amount) AS mean_tip\nFROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\nGROUP BY passenger_count\n\n<PLAN>\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             #0            │\n│          avg(#1)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│      passenger_count      │\n│         tip_amount        │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│      passenger_count      │\n│         tip_amount        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│       EC: 179629584       │\n└───────────────────────────┘                             \n```\n\n\n:::\n:::\n\n\nSimilarly, to show the SQL translation that will be implemented on the backend,\nusing `show_query`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_query(q1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<SQL>\nSELECT passenger_count, AVG(tip_amount) AS mean_tip\nFROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\nGROUP BY passenger_count\n```\n\n\n:::\n:::\n\n\nNote that printing the query object actually does enforce some computation.\nOTOH it's still just a preview of the data (we haven't pulled everything into\nR's memory).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Source:   SQL [?? x 2]\n# Database: DuckDB v0.10.1 [gmcd@Darwin 23.4.0:R 4.4.0/:memory:]\n   passenger_count mean_tip\n             <dbl>    <dbl>\n 1               5    1.10 \n 2              65    0    \n 3               9    0.807\n 4             177    1    \n 5               0    0.862\n 6             254    0    \n 7             249    0    \n 8               7    0.544\n 9               8    0.351\n10              66    1.5  \n# ℹ more rows\n```\n\n\n:::\n:::\n\n\nTo actually pull all of the result data into R, we must call `collect()`\non the query object\n\n::: {.cell}\n\n```{.r .cell-code}\ntic = Sys.time()\ndat1 = collect(q1)\ntoc = Sys.time()\n\ndat1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 × 2\n   passenger_count mean_tip\n             <dbl>    <dbl>\n 1               0    0.862\n 2             254    0    \n 3             249    0    \n 4               5    1.10 \n 5              65    0    \n 6               9    0.807\n 7             177    1    \n 8               2    1.08 \n 9             208    0    \n10              10    0    \n11               4    0.845\n12               1    1.15 \n13             247    2.3  \n14               3    0.963\n15               6    1.13 \n16               7    0.544\n17               8    0.351\n18              66    1.5  \n```\n\n\n:::\n\n```{.r .cell-code}\ntoc - tic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 1.2924 secs\n```\n\n\n:::\n:::\n\n\n\n\n## Aggregation\n\nHere's our earlier filtering example with multiple grouping + aggregation\nvariables...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq2 = nyc |>\n  filter(month <= 3) |>\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n    .by = c(month, passenger_count)\n  )\nq2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Source:   SQL [?? x 4]\n# Database: DuckDB v0.10.1 [gmcd@Darwin 23.4.0:R 4.4.0/:memory:]\n   month passenger_count tip_amount fare_amount\n   <dbl>           <dbl>      <dbl>       <dbl>\n 1     1               1      1.04         9.76\n 2     2               1      1.07         9.90\n 3     3               1      1.09        10.2 \n 4     1               8      0           21.3 \n 5     2               8      0.5          8.23\n 6     1               7      0            6.3 \n 7     1               3      0.875        9.87\n 8     1               6      1.02         9.86\n 9     2               3      0.895        9.98\n10     2               6      1.02         9.96\n# ℹ more rows\n```\n\n\n:::\n:::\n\n\nAside: note the optimised query includes hash groupings and projection\n(basically: fancy column subsetting, which is a suprisingly effective strategy\nin query optimization)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexplain(q2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<SQL>\nSELECT\n  \"month\",\n  passenger_count,\n  AVG(tip_amount) AS tip_amount,\n  AVG(fare_amount) AS fare_amount\nFROM (\n  SELECT q01.*\n  FROM (FROM read_parquet('nyc-taxi/**/*.parquet', hive_partitioning = true)) q01\n  WHERE (\"month\" <= 3.0)\n) q01\nGROUP BY \"month\", passenger_count\n\n<PLAN>\nphysical_plan\n┌───────────────────────────┐\n│       HASH_GROUP_BY       │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│             #0            │\n│             #1            │\n│          avg(#2)          │\n│          avg(#3)          │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│           month           │\n│      passenger_count      │\n│         tip_amount        │\n│        fare_amount        │\n└─────────────┬─────────────┘                             \n┌─────────────┴─────────────┐\n│       READ_PARQUET        │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│      passenger_count      │\n│        fare_amount        │\n│         tip_amount        │\n│           month           │\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│ File Filters: (month <= 3)│\n│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │\n│        EC: 44907396       │\n└───────────────────────────┘                             \n```\n\n\n:::\n:::\n\n\nAnd our high-dimensional aggregation example. We'll create a query for this\nfirst, since I'll reuse it shortly again\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq3 = nyc |>\n  group_by(passenger_count, trip_distance) |>\n  summarize(\n    across(c(tip_amount, fare_amount), mean),\n  ) \ncollect(q3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25,569 × 4\n# Groups:   passenger_count [18]\n   passenger_count trip_distance tip_amount fare_amount\n             <dbl>         <dbl>      <dbl>       <dbl>\n 1               1          0.7       0.493        5.04\n 2               2          0.8       0.462        5.44\n 3               1          0.8       0.535        5.36\n 4               1          1         0.616        6.01\n 5               1          0.3       0.334        4.11\n 6               1          4.8       1.70        16.0 \n 7               2          1         0.525        6.08\n 8               1          6.7       2.13        20.1 \n 9               1          0.4       0.361        4.17\n10               2          0.98      0.542        5.85\n# ℹ 25,559 more rows\n```\n\n\n:::\n:::\n\n\n## Pivot (reshape)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(tidyr) ## already loaded\n\nq3 |>\n  pivot_longer(tip_amount:fare_amount) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 51,138 × 4\n# Groups:   passenger_count [18]\n   passenger_count trip_distance name       value\n             <dbl>         <dbl> <chr>      <dbl>\n 1               1           2.2 tip_amount 1.01 \n 2               1          16.8 tip_amount 5.58 \n 3               1           2.7 tip_amount 1.16 \n 4               1           5.9 tip_amount 1.93 \n 5               1           8.4 tip_amount 2.93 \n 6               0           2.4 tip_amount 0.924\n 7               1           3.8 tip_amount 1.46 \n 8               1           5.4 tip_amount 1.83 \n 9               1           9.3 tip_amount 3.40 \n10               1           9.8 tip_amount 3.60 \n# ℹ 51,128 more rows\n```\n\n\n:::\n:::\n\n\n## Joins (merges)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_tips  = nyc |> summarise(mean_tips = mean(tip_amount), .by = month)\nmean_fares = nyc |> summarise(mean_fares = mean(fare_amount), .by = month)\n```\n:::\n\n\nAgain, these commands complete instantly because all computation has been\ndeferred until absolutely necessary (i.e.,. lazy eval).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleft_join(\n  mean_fares,\n  mean_tips\n  ) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(month)`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 × 3\n   month mean_fares mean_tips\n   <dbl>      <dbl>     <dbl>\n 1    11      12.3       1.25\n 2     7      10.4       1.06\n 3     8      10.5       1.08\n 4     1       9.81      1.01\n 5     4      10.3       1.04\n 6     5      10.6       1.08\n 7     9      12.4       1.25\n 8    10      12.5       1.28\n 9     2       9.94      1.04\n10    12      12.3       1.24\n11     3      10.2       1.06\n12     6      10.5       1.09\n```\n\n\n:::\n:::\n\n\n## Windowing\n\nIf you recall from the native SQL API, we sampled 1 percent of the data before\ncreating decile bins to reduce the computation burden of sorting the entire\ntable. Unfortunately, this approach doesn't work as well for the **dplyr**\nfrontend because the underlying SQL translation\n[uses](https://dbplyr.tidyverse.org/reference/dbplyr-slice.html) a generic\nsampling approach (rather than DuckDB's optimised `USING SAMPLE` statement.)\n\n## Close connection\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(con)\n```\n:::\n\n\n## Appendix: Related interfaces\n\n### arrow+duckdb\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nlibrary(duckdb) ## Already loaded\nlibrary(dplyr)  ## Already loaded\nlibrary(tidyr)  ## Already loaded\n```\n:::\n\n\nWhen going through the **arrow** intermediary, we don't need to establish a\ndatabase with `DBI::dbConnect` like we did above. Instead, we can create a link\n(pointers) to the dataset on disk directly via the `arrow::open_dataset()`\nconvience function. Here I'll assign it to a new R object called `nyc2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc2 = open_dataset(\"nyc-taxi\")\n```\n:::\n\n\n:::{.callout-tip}\n## open_dataset() versus read_parquet()\n(For individual parquet files, we could just read then via\n`arrow::read_parquet()`, perhaps efficiently subsetting columns at the same\ntime. But I find the `open_dataset` is generally what I'm looking for.)\n:::\n\nNote that printing our `nyc2` dataset to the R console will just display the\ndata schema. This is a cheap and convenient way to quickly interrogate the basic\nstructure of your data, including column types, etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFileSystemDataset with 12 Parquet files\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\npickup_location_id: int64\ndropoff_location_id: int64\nyear: int32\nmonth: int32\n```\n\n\n:::\n:::\n\n\nThe key step for this \"arrow + duckdb\" **dplyr** workflow is to pass our arrow\ndataset to DuckDB via the `to_duckdb()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_duckdb(nyc2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Source:   table<arrow_001> [?? x 24]\n# Database: DuckDB v0.10.1 [root@Darwin 23.4.0:R 4.4.0/:memory:]\n   vendor_name pickup_datetime     dropoff_datetime    passenger_count\n   <chr>       <dttm>              <dttm>                        <dbl>\n 1 CMT         2012-01-20 14:09:36 2012-01-20 14:42:25               1\n 2 CMT         2012-01-20 14:54:10 2012-01-20 15:06:55               1\n 3 CMT         2012-01-20 08:08:01 2012-01-20 08:11:02               1\n 4 CMT         2012-01-20 08:36:22 2012-01-20 08:39:44               1\n 5 CMT         2012-01-20 20:58:32 2012-01-20 21:03:04               1\n 6 CMT         2012-01-20 19:40:20 2012-01-20 19:43:43               2\n 7 CMT         2012-01-21 01:54:37 2012-01-21 02:08:02               2\n 8 CMT         2012-01-21 01:55:47 2012-01-21 02:08:51               3\n 9 VTS         2012-01-07 22:20:00 2012-01-07 22:27:00               2\n10 VTS         2012-01-10 07:11:00 2012-01-10 07:21:00               1\n# ℹ more rows\n# ℹ 20 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <dbl>, …\n```\n\n\n:::\n:::\n\n\nNote that this transfer from Arrow to DuckDB is very quick (and memory cheap)\nbecause it is a zero copy. We are just passing around pointers instead of\nactually moving any data. See\n[this blog post](https://duckdb.org/2021/12/03/duck-arrow.html)\nfor more details, but the high-level take away is that we are benefitting from\nthe tightly integrated architectures of these two libraries.^[\"Similar\" might be\na better description than \"integrated\", since DuckdB does not use the Arrow\nmemory model. But they are both columnar-orientated (among other things) and so\nthe end result is pretty seamless integration.]\n\nAt this, point all of the regular **dplyr** workflow logic from above should\ncarry over. Just remember to first pass the arrow dataset via the `to_duckdb()`\nfunciton. For example, here's our initial aggregation query again:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc2 |>\n  to_duckdb() |> ## <= key step\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  ) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 × 2\n   passenger_count mean_tip\n             <dbl>    <dbl>\n 1               5    1.10 \n 2               9    0.807\n 3              65    0    \n 4             177    1    \n 5               7    0.544\n 6               8    0.351\n 7              66    1.5  \n 8               1    1.15 \n 9             247    2.3  \n10               4    0.845\n11               0    0.862\n12             254    0    \n13             249    0    \n14               2    1.08 \n15             208    0    \n16              10    0    \n17               3    0.963\n18               6    1.13 \n```\n\n\n:::\n:::\n\n\n:::{.callout-note}\n## Arrow's native acero engine\n\nSome of you may be used to performing computation with the **arrow** package\nwithout going through DuckDB. What's happening here is that arrow provides its\nown computation engine called \"acero\". This Arrow-native engine is actually\npretty performant... albeit not a fast as DuckDB, nor as feature rich. So I\npersonally recommend always passing to DuckDB if you can. Still, if you're\ncurious then you can test yourself by re-trying the code chunk, but commenting\nout the `to_duckdb()` line. For more details, see\n[here](https://youtu.be/LvTX1ZAZy6M?si=7gZYG03ojtAtPGfe).\n:::\n\n### duckplyr\n\nThe new kid on the block is **duckplyr**\n([announcement](https://duckdb.org/2024/04/02/duckplyr.html) /\n[homepage](https://duckdblabs.github.io/duckplyr/)).\nWithout going into too much depth, the promise of **duckplyr** is that it can\nprovide a \"fully native\" dplyr experience that is _directly_ coupled to DuckDB's\nquery engine. So, for example, it won't have to rely on **DBI**'s generic' SQL\ntranslations. Instead, the relevant **dplyr** \"verbs\" are being directly\ntranslated to DuckDB's relational API to construct logical query plans. If\nthat's too much jargon, just know that it should involve less overhead, fewer\ntranslation errors, and better optimization. Moreover, a goal of **duckplyr** is\nfor it to be a drop-in replace for **dplyr** _in general_. In other words, you\ncould just swap out `library(dplyr)` for `library(duckplyr)` and all of your\ndata wrangling operations will come backed by the power of DuckDB. This includes\nfor working on \"regular\" R data frames in memory.\n\nAll of this is exciting and I would urge you stay tuned. Right now, **duckplyr**\nis still marked as experimental and has a few rough edges. But the basics are\nthere. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(duckplyr, warn.conflicts = FALSE)\n\nduckplyr_df_from_parquet(\"nyc-taxi/**/*.parquet\") |>\n  summarise(\n    mean_tip = mean(tip_amount),\n    .by = passenger_count\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmaterializing:\n---------------------\n--- Relation Tree ---\n---------------------\nAggregate [passenger_count, mean(tip_amount)]\n  read_parquet(nyc-taxi/**/*.parquet)\n\n---------------------\n-- Result Columns  --\n---------------------\n- passenger_count (BIGINT)\n- mean_tip (DOUBLE)\n\n   passenger_count  mean_tip\n1                2 1.0815798\n2              208 0.0000000\n3               10 0.0000000\n4                5 1.1027325\n5                9 0.8068000\n6               65 0.0000000\n7              177 1.0000000\n8                3 0.9629494\n9                6 1.1283649\n10               0 0.8620988\n11             249 0.0000000\n12             254 0.0000000\n13               8 0.3507692\n14               7 0.5441176\n15              66 1.5000000\n16               1 1.1510110\n17             247 2.3000000\n18               4 0.8445190\n```\n\n\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}